import { type ActionFunctionArgs } from '@remix-run/cloudflare';
import { createUIMessageStream, createUIMessageStreamResponse, generateId } from 'ai';
import { MAX_RESPONSE_SEGMENTS, MAX_TOKENS, isReasoningModel, type FileMap } from '~/lib/.server/llm/constants';
import { CONTINUE_PROMPT } from '~/lib/common/prompts/prompts';
import { streamText, type Messages, type StreamingOptions } from '~/lib/.server/llm/stream-text';
import type { IProviderSetting } from '~/types/model';
import { createScopedLogger } from '~/utils/logger';
import { getFilePaths } from '~/lib/.server/llm/select-context';
import { REASONING_ANNOTATION_TYPE, USAGE_ANNOTATION_TYPE, type ReasoningAnnotation } from '~/types/context';
import { extractPropertiesFromMessage } from '~/lib/.server/llm/utils';
import type { DesignScheme } from '~/types/design-scheme';
import { MCPService } from '~/lib/services/mcpService';
import { StreamRecoveryManager } from '~/lib/.server/llm/stream-recovery';
import { getReasoningSummary } from '~/lib/modules/llm/providers/azure-openai';

export async function action(args: ActionFunctionArgs) {
  return chatAction(args);
}

const logger = createScopedLogger('api.chat');

function parseCookies(cookieHeader: string): Record<string, string> {
  const cookies: Record<string, string> = {};

  const items = cookieHeader.split(';').map((cookie) => cookie.trim());

  items.forEach((item) => {
    const [name, ...rest] = item.split('=');

    if (name && rest) {
      const decodedName = decodeURIComponent(name.trim());
      const decodedValue = decodeURIComponent(rest.join('=').trim());
      cookies[decodedName] = decodedValue;
    }
  });

  return cookies;
}

async function chatAction({ context, request }: ActionFunctionArgs) {
  const streamRecovery = new StreamRecoveryManager({
    timeout: 180000, // 3 minutes - Azure Responses API needs time for reasoning on large projects
    maxRetries: 2,
    onTimeout: () => {
      logger.warn('Stream timeout - attempting recovery');
    },
  });
  let responseSegments = 0;

  const { messages, files, promptId, contextOptimization, supabase, chatMode, designScheme, webSearchEnabled } =
    await request.json<{
      messages: Messages;
      files: any;
      promptId?: string;
      contextOptimization: boolean;
      chatMode: 'discuss' | 'build';
      designScheme?: DesignScheme;
      supabase?: {
        isConnected: boolean;
        hasSelectedProject: boolean;
        credentials?: {
          anonKey?: string;
          supabaseUrl?: string;
        };
      };
      maxLLMSteps: number;
      webSearchEnabled?: boolean;
    }>();

  const cookieHeader = request.headers.get('Cookie');
  const apiKeys = JSON.parse(parseCookies(cookieHeader || '').apiKeys || '{}');
  const providerSettings: Record<string, IProviderSetting> = JSON.parse(
    parseCookies(cookieHeader || '').providers || '{}',
  );

  const cumulativeUsage = {
    completionTokens: 0,
    promptTokens: 0,
    totalTokens: 0,
  };
  let progressCounter: number = 1;

  try {
    const mcpService = MCPService.getInstance();
    const totalMessageContent = messages.reduce((acc, message) => {
      const textContent = message.parts
        .filter((part) => part.type === 'text')
        .map((part) => part.text)
        .join('');
      return acc + textContent;
    }, '');
    logger.debug(`Total message length: ${totalMessageContent.split(' ').length}, words`);

    logger.debug(`Total message length: ${totalMessageContent.split(' ').length}, words`);

    const stream = createUIMessageStream({
      async execute({ writer }) {
        streamRecovery.startMonitoring();

        const filePaths = getFilePaths(files || {});
        const filteredFiles: FileMap | undefined = undefined;
        const summary: string | undefined = undefined;
        let messageSliceId = 0;

        const processedMessages = await mcpService.processToolInvocations(messages, writer);

        if (processedMessages.length > 3) {
          messageSliceId = processedMessages.length - 3;
        }

        if (filePaths.length > 0 && contextOptimization) {
          logger.debug('Generating Chat Summary');
          writer.write({
            type: 'data-progress',
            label: 'summary',
            status: 'in-progress',
            order: progressCounter++,
            message: 'Analysing Request',
          } as any);

          // ï¼ˆå·²ç§»é™¤éŒ¯èª¤æ’å…¥çš„æŽ¨ç†è¿´åœˆï¼Œsummary é‚è¼¯ä¿æŒåŽŸæ¨£ï¼‰
        }

        const options: StreamingOptions = {
          supabaseConnection: supabase,
          toolChoice: 'auto',
          tools: mcpService.toolsWithoutExecute,

          // maxSteps: maxLLMSteps,
          onStepFinish: ({ toolCalls }) => {
            toolCalls.forEach((toolCall) => {
              mcpService.processToolCall(toolCall as any, writer);
            });
          },
          onFinish: async (result: any) => {
            const { text: content, finishReason, usage, response } = result;
            const experimentalProviderMetadata = (result as any).experimental_providerMetadata;
            logger.info('[onFinish] ========== CALLBACK CALLED ==========');

            // Mark analysis as complete if it was started
            if (filePaths.length > 0 && contextOptimization) {
              writer.write({
                type: 'data-progress',
                label: 'summary',
                status: 'complete',
                order: progressCounter++,
                message: 'Analysis Complete',
              } as any);
            }

            logger.debug('usage', JSON.stringify(usage));
            logger.debug('finishReason', finishReason);
            logger.debug('experimentalProviderMetadata', JSON.stringify(experimentalProviderMetadata));
            logger.debug('response keys', response ? Object.keys(response) : 'no response');

            // æå– reasoning summaryï¼ˆå®˜æ–¹æŽ¨è–¦æ–¹å¼ï¼‰
            let reasoningSummary: string | undefined;
            let reasoningContent: string | undefined;

            try {
              // ðŸ”¥ æ–¹æ³• 1 (æœ€å„ªå…ˆ): å¾žå…¨å±€ Map è®€å–æŽ¨ç†æ‘˜è¦ï¼ˆé€šéŽ request IDï¼‰
              if (response && response.headers) {
                let requestId: string | null = null;

                // æª¢æŸ¥ headers æ˜¯å¦æ˜¯ Headers å¯¦ä¾‹æˆ–æ™®é€šå°è±¡
                if (typeof (response.headers as any).get === 'function') {
                  // æ¨™æº– Headers ç‰©ä»¶
                  requestId = (response.headers as any).get('x-reasoning-request-id');
                } else if (typeof response.headers === 'object') {
                  // æ™®é€šå°è±¡
                  requestId = (response.headers as any)['x-reasoning-request-id'] || null;
                }

                if (requestId) {
                  logger.info('[Reasoning] ðŸ” Found x-reasoning-request-id:', requestId);

                  const azureData = getReasoningSummary(requestId);

                  if (azureData) {
                    logger.info('[Reasoning] âœ…âœ…âœ… å¾žå…¨å±€ Map ä¸­æ‰¾åˆ°æŽ¨ç†æ‘˜è¦ï¼');

                    if (azureData.summary && typeof azureData.summary === 'string') {
                      reasoningSummary = azureData.summary;
                      logger.info('[Reasoning] Summary é•·åº¦:', reasoningSummary.length);
                      logger.debug('[Reasoning] Summary é–‹é ­:', reasoningSummary.substring(0, 200));
                    } else if (azureData.encrypted && typeof azureData.encrypted === 'string') {
                      // å¦‚æžœåªæœ‰åŠ å¯†å…§å®¹ï¼Œå‰µå»ºä¸€å€‹èªªæ˜Žè¨Šæ¯
                      reasoningSummary = `ðŸ” æŽ¨ç†å…§å®¹å·²åŠ å¯†\n\næ­¤å›žæ‡‰åŒ…å«åŠ å¯†çš„æŽ¨ç†å…§å®¹ï¼ˆ${azureData.encrypted.length} å­—ç¬¦ï¼‰ã€‚Azure OpenAI æä¾›çš„æŽ¨ç†å…§å®¹æ˜¯åŠ å¯†æ ¼å¼ï¼Œç›®å‰ç„¡æ³•ç›´æŽ¥é¡¯ç¤ºåŽŸå§‹æ€è€ƒéŽç¨‹ã€‚\n\nä½†æ˜¯ï¼Œæ¨¡åž‹çš„æŽ¨ç†éŽç¨‹å·²ç¶“å®Œæˆï¼Œä¸¦åæ˜ åœ¨æœ€çµ‚çš„å›žæ‡‰ä¸­ã€‚`;
                      logger.info('[Reasoning] âš ï¸ åªæ‰¾åˆ°åŠ å¯†çš„æŽ¨ç†å…§å®¹ï¼Œå‰µå»ºèªªæ˜Žè¨Šæ¯');
                    }
                  } else {
                    logger.warn('[Reasoning] âš ï¸ å¾žå…¨å±€ Map ä¸­æœªæ‰¾åˆ°æŽ¨ç†æ•¸æ“šï¼Œrequest ID:', requestId);
                  }
                } else {
                  logger.debug('[Reasoning] æœªæ‰¾åˆ° x-reasoning-request-id header');
                }
              }

              // æ–¹æ³• 2: å¾ž response ç‰©ä»¶æå–å…¶ä»–å±¬æ€§ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
              if (!reasoningSummary && !reasoningContent && response) {
                logger.debug('[Reasoning] Attempting to extract from response object properties');

                const responseObj = response as any;

                if (responseObj.reasoning) {
                  reasoningContent = responseObj.reasoning;
                  logger.info('[Reasoning] âœ… Found reasoning in response.reasoning');
                } else if (responseObj.reasoningSummary) {
                  reasoningSummary = responseObj.reasoningSummary;
                  logger.info('[Reasoning] âœ… Found reasoningSummary in response.reasoningSummary');
                } else if (responseObj.headers) {
                  // å˜—è©¦å¾ž headers ä¸­æå–
                  logger.debug('[Reasoning] Checking response headers for reasoning content');

                  const headers = responseObj.headers;

                  if (headers && typeof headers.get === 'function') {
                    const reasoningHeader = headers.get('x-reasoning-summary') || headers.get('reasoning-summary');

                    if (reasoningHeader) {
                      reasoningSummary = reasoningHeader;
                      logger.info('[Reasoning] âœ… Found reasoning in response headers');
                    }
                  }
                }
              }

              // æ–¹æ³• 2: å¾ž experimentalProviderMetadata æå–ï¼ˆVercel AI SDK å®˜æ–¹æ–¹å¼ï¼‰
              if (!reasoningSummary && !reasoningContent) {
                if (experimentalProviderMetadata?.azure?.reasoningSummary) {
                  reasoningSummary = String(experimentalProviderMetadata.azure.reasoningSummary);
                  logger.info('[Reasoning] âœ… Found reasoningSummary in experimentalProviderMetadata.azure');
                } else if (experimentalProviderMetadata?.openai?.reasoningSummary) {
                  reasoningSummary = String(experimentalProviderMetadata.openai.reasoningSummary);
                  logger.info('[Reasoning] âœ… Found reasoningSummary in experimentalProviderMetadata.openai');
                }
              }

              // æ–¹æ³• 3: æª¢æŸ¥æ˜¯å¦æœ‰ reasoningTokensï¼ˆè¡¨ç¤ºæ¨¡åž‹ä½¿ç”¨äº†æŽ¨ç†ä½†å…§å®¹æœªæå–ï¼‰
              if (!reasoningSummary && !reasoningContent) {
                const reasoningTokens = experimentalProviderMetadata?.openai?.reasoningTokens;
                const hasReasoningTokens = typeof reasoningTokens === 'number' && reasoningTokens > 0;

                if (hasReasoningTokens) {
                  logger.warn(`[Reasoning] âš ï¸ æ¨¡åž‹ä½¿ç”¨äº† ${reasoningTokens} å€‹æŽ¨ç† tokensï¼Œä½†æœªèƒ½æå–æŽ¨ç†å…§å®¹`);
                  logger.warn('[Reasoning] é€™å¯èƒ½æ˜¯ AI SDK v5 å° Azure Responses API çš„æ”¯æ´å•é¡Œ');

                  // å‰µå»ºä¸€å€‹æç¤ºä¿¡æ¯
                  reasoningSummary = `æ­¤å›žæ‡‰ä½¿ç”¨äº† ${reasoningTokens} å€‹æŽ¨ç† tokens é€²è¡Œæ·±åº¦æ€è€ƒã€‚\n\næ³¨æ„ï¼šæŽ¨ç†éŽç¨‹å·²åœ¨ç”Ÿæˆå›žæ‡‰æ™‚å®Œæˆï¼Œä½†è©³ç´°å…§å®¹æš«æ™‚ç„¡æ³•å®Œæ•´æå–ã€‚`;
                }
              }

              // å¦‚æžœæ‰¾åˆ° reasoning å…§å®¹ï¼Œç™¼é€åˆ°å‰ç«¯
              const finalReasoningContent = reasoningSummary || reasoningContent;

              if (finalReasoningContent) {
                logger.info('[Reasoning] âœ… Reasoning content found, length:', finalReasoningContent.length);
                logger.debug('[Reasoning] Content preview:', finalReasoningContent.substring(0, 200));

                // ç™¼é€ reasoning å…§å®¹ä½œç‚ºè‡ªå®šç¾©è¨»è§£ï¼ˆçµ±ä¸€æ ¼å¼ï¼‰
                const reasoningAnnotation: ReasoningAnnotation = {
                  type: REASONING_ANNOTATION_TYPE,
                  summary: finalReasoningContent,
                  provider: experimentalProviderMetadata?.azure
                    ? 'azure'
                    : experimentalProviderMetadata?.openai
                      ? 'openai'
                      : undefined,
                  model: response?.modelId,
                };

                writer.write({
                  ...reasoningAnnotation,
                  type: `data-${reasoningAnnotation.type}`,
                } as any);

                logger.info('[Reasoning] âœ… Reasoning annotation sent to frontend');
              } else {
                logger.warn('[Reasoning] âš ï¸ No reasoning content found in response');
                logger.warn(
                  '[Reasoning] Available metadata:',
                  JSON.stringify(
                    {
                      hasAzureMetadata: !!experimentalProviderMetadata?.azure,
                      hasOpenAIMetadata: !!experimentalProviderMetadata?.openai,
                      azureKeys: experimentalProviderMetadata?.azure
                        ? Object.keys(experimentalProviderMetadata.azure)
                        : [],
                      openaiKeys: experimentalProviderMetadata?.openai
                        ? Object.keys(experimentalProviderMetadata.openai)
                        : [],
                      reasoningTokens: experimentalProviderMetadata?.openai?.reasoningTokens,
                    },
                    null,
                    2,
                  ),
                );
              }
            } catch (error) {
              logger.error('[Reasoning] Error extracting reasoning content:', error);
            }

            if (usage) {
              cumulativeUsage.completionTokens += (usage as any).completionTokens || 0;
              cumulativeUsage.promptTokens += (usage as any).promptTokens || 0;
              cumulativeUsage.totalTokens += (usage as any).totalTokens || 0;
            }

            if (finishReason !== 'length') {
              writer.write({
                type: `data-${USAGE_ANNOTATION_TYPE}`,
                value: {
                  completionTokens: cumulativeUsage.completionTokens,
                  promptTokens: cumulativeUsage.promptTokens,
                  totalTokens: cumulativeUsage.totalTokens,
                },
              } as any);
              writer.write({
                type: 'data-progress',
                label: 'response',
                status: 'complete',
                order: progressCounter++,
                message: 'Response Generated',
              } as any);
              streamRecovery.stop();
              await new Promise((resolve) => setTimeout(resolve, 0));

              return;
            }

            if (responseSegments >= MAX_RESPONSE_SEGMENTS) {
              streamRecovery.stop();
              throw Error('Cannot continue message: Maximum segments reached');
            }

            const switchesLeft = MAX_RESPONSE_SEGMENTS - responseSegments;
            logger.info(`Reached max token limit (${MAX_TOKENS}): Continuing message (${switchesLeft} switches left)`);

            const lastUserMessage = processedMessages.filter((x) => x.role === 'user').slice(-1)[0];
            const { model, provider } = extractPropertiesFromMessage(lastUserMessage);
            processedMessages.push({ id: generateId(), role: 'assistant', parts: [{ type: 'text', text: content }] });
            processedMessages.push({
              id: generateId(),
              role: 'user',
              parts: [
                {
                  type: 'text',
                  text: `[Model: ${model}]\n\n[Provider: ${provider}]\n\n${CONTINUE_PROMPT}`,
                },
              ],
            });

            responseSegments += 1;

            const continuationResult = await streamText({
              messages: [...processedMessages] as any,
              env: context.cloudflare?.env,
              options,
              apiKeys,
              files,
              providerSettings,
              promptId,
              contextOptimization,
              contextFiles: filteredFiles,
              chatMode,
              designScheme,
              summary,
              messageSliceId,
              webSearchEnabled,
            });

            // Monitor fullStream in continuation
            (async () => {
              try {
                for await (const part of (continuationResult as any).fullStream) {
                  streamRecovery.updateActivity();

                  if (part.type === 'error') {
                    const error: any = part.error;
                    logger.error('Continuation streaming error:', error);
                    streamRecovery.stop();

                    return;
                  }
                }

                streamRecovery.stop();
              } catch (error) {
                logger.error('Error in continuation fullStream monitoring:', error);
                streamRecovery.stop();
              }
            })();

            // ä½¿ç”¨ AI SDK æ­£ç¢ºçš„æµåˆä½µæ–¹æ³•ï¼Œä¸¦å•Ÿç”¨æŽ¨ç†å…§å®¹å‚³è¼¸
            writer.merge((continuationResult as any).toUIMessageStream());
          },
        };

        // æª¢æ¸¬æ˜¯å¦ç‚º reasoning modelï¼ˆç”¨æ–¼é€²åº¦æ¶ˆæ¯ï¼‰
        const lastUserMessage = processedMessages.filter((x) => x.role === 'user').slice(-1)[0];
        const { model: selectedModel } = extractPropertiesFromMessage(lastUserMessage);
        const isReasoning = isReasoningModel(selectedModel);

        /*
         * ä¸å†æå‰ç™¼é€ã€Œæ€è€ƒä¸­ã€æç¤ºï¼Œç­‰å¾…çœŸæ­£çš„æŽ¨ç†å…§å®¹å¾ž onFinish è¿”å›ž
         * é€™æ¨£å¯ä»¥é¿å…é¡¯ç¤ºä½”ä½ç¬¦æ–‡å­—
         */

        writer.write({
          type: 'data-progress',
          label: 'response',
          status: 'in-progress',
          order: progressCounter++,
          message: isReasoning ? 'AI æ·±åº¦æ€è€ƒä¸­...' : 'Generating Response',
        } as any);

        const result = await streamText({
          messages: [...processedMessages] as any,
          env: context.cloudflare?.env,
          options,
          apiKeys,
          files,
          providerSettings,
          promptId,
          contextOptimization,
          contextFiles: filteredFiles,
          chatMode,
          designScheme,
          summary,
          messageSliceId,
          webSearchEnabled,
        });

        // Monitor fullStream to update recovery activity and handle errors
        (async () => {
          try {
            for await (const part of result.fullStream) {
              streamRecovery.updateActivity();

              if (part.type === 'error') {
                const error: any = part.error;
                logger.error('Streaming error:', error);
                streamRecovery.stop();

                // Enhanced error handling for common streaming issues
                if (error.message?.includes('Invalid JSON response')) {
                  logger.error('Invalid JSON response detected - likely malformed API response');
                } else if (error.message?.includes('token')) {
                  logger.error('Token-related error detected - possible token limit exceeded');
                }

                return;
              }
            }

            streamRecovery.stop();
          } catch (error) {
            logger.error('Error in fullStream monitoring:', error);
            streamRecovery.stop();
          }
        })();

        // ä½¿ç”¨ AI SDK æ­£ç¢ºçš„æµåˆä½µæ–¹æ³•ï¼Œä¸¦å•Ÿç”¨æŽ¨ç†å…§å®¹å‚³è¼¸
        writer.merge(result.toUIMessageStream());
      },
      onError: (error: any) => {
        streamRecovery.stop();

        // Provide more specific error messages for common issues
        const errorMessage = error.message || 'Unknown error';

        if (errorMessage.includes('model') && errorMessage.includes('not found')) {
          return 'Custom error: Invalid model selected. Please check that the model name is correct and available.';
        }

        if (errorMessage.includes('Invalid JSON response')) {
          return 'Custom error: The AI service returned an invalid response. This may be due to an invalid model name, API rate limiting, or server issues. Try selecting a different model or check your API key.';
        }

        if (
          errorMessage.includes('API key') ||
          errorMessage.includes('unauthorized') ||
          errorMessage.includes('authentication')
        ) {
          return 'Custom error: Invalid or missing API key. Please check your API key configuration.';
        }

        if (errorMessage.includes('token') && errorMessage.includes('limit')) {
          return 'Custom error: Token limit exceeded. The conversation is too long for the selected model. Try using a model with larger context window or start a new conversation.';
        }

        if (errorMessage.includes('rate limit') || errorMessage.includes('429')) {
          return 'Custom error: API rate limit exceeded. Please wait a moment before trying again.';
        }

        if (errorMessage.includes('network') || errorMessage.includes('timeout')) {
          return 'Custom error: Network error. Please check your internet connection and try again.';
        }

        return `Custom error: ${errorMessage}`;
      },
    });

    return createUIMessageStreamResponse({ stream });
  } catch (error: any) {
    streamRecovery.stop();
    logger.error(error);

    const errorResponse = {
      error: true,
      message: error.message || 'An unexpected error occurred',
      statusCode: error.statusCode || 500,
      isRetryable: error.isRetryable !== false, // Default to retryable unless explicitly false
      provider: error.provider || 'unknown',
    };

    if (error.message?.includes('API key')) {
      return new Response(
        JSON.stringify({
          ...errorResponse,
          message: 'Invalid or missing API key',
          statusCode: 401,
          isRetryable: false,
        }),
        {
          status: 401,
          headers: { 'Content-Type': 'application/json' },
          statusText: 'Unauthorized',
        },
      );
    }

    return new Response(JSON.stringify(errorResponse), {
      status: errorResponse.statusCode,
      headers: { 'Content-Type': 'application/json' },
      statusText: 'Error',
    });
  }
}
